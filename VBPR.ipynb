{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import heapq\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.decomposition import PCA\n",
    "from utils import load_embeddings_and_ids, User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a single GPU because we want to be nice with other people :)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load pre-trained ResNet50 image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_embeddings,\\\n",
    "artwork_ids,\\\n",
    "artwork_id2index = load_embeddings_and_ids(\n",
    "'/mnt/workspace/Ugallery/ResNet50/', 'flatten_1.npy', 'ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13297"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_artworks = len(artwork_ids)\n",
    "n_artworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Load transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('./valid_sales.csv')\n",
    "artworks_df = pd.read_csv('./valid_artworks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_ids = np.full((n_artworks,), -1, dtype=int)\n",
    "for _artworkId, _artistId in zip(artworks_df.id, artworks_df.artist_id):\n",
    "    i = artwork_id2index[_artworkId]\n",
    "    artist_ids[i] = _artistId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "artistId2artworkIndexes = dict()\n",
    "for i, _artistId in enumerate(artist_ids):\n",
    "    if _artistId == -1:\n",
    "        continue\n",
    "    try:\n",
    "        artistId2artworkIndexes[_artistId].append(i)\n",
    "    except KeyError:\n",
    "        artistId2artworkIndexes[_artistId] = [i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect transactions per user (making sure we hide the last nonfirst purchase basket per user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create list of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids = sales_df.customer_id.unique()\n",
    "user_id2index = { _id:i for i,_id in enumerate(user_ids) }\n",
    "users = [User(uid) for uid in user_ids]\n",
    "n_users = len(user_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect and sanity check transactions per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sales_df = sales_df.sort_values('order_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear structures to prevent possible duplicate elements\n",
    "for user in users:\n",
    "    user.clear()\n",
    "\n",
    "# collect transactions per user sorted by timestamp\n",
    "for uid, aid, t in zip(sorted_sales_df.customer_id,\n",
    "                       sorted_sales_df.artwork_id,\n",
    "                       sorted_sales_df.order_date):\n",
    "    users[user_id2index[uid]].append_transaction(aid,t,artwork_id2index,artist_ids)\n",
    "    assert users[user_id2index[uid]]._uid == uid\n",
    "    \n",
    "# bin transctions with same timestamps into purchase baskets\n",
    "for user in users:\n",
    "    user.build_purchase_baskets()\n",
    "    user.sanity_check_purchase_baskets()\n",
    "    user.remove_last_nonfirst_purchase_basket(artwork_id2index, artist_ids)\n",
    "    user.sanity_check_purchase_baskets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute minimun cosine distance from each user profile to each item in the dataset\n",
    "\\* using R200 vectors obtained with PCA(200) over ResNet50 embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50_PCA200 = PCA(n_components=200).fit_transform(resnet50_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13297, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50_PCA200.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "distmat = squareform(pdist(resnet50_PCA200, 'cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2artwork_mindist = np.empty((n_users, n_artworks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2919/2919 [00:42<00:00, 69.41it/s]\n"
     ]
    }
   ],
   "source": [
    "for ui in tqdm(range(n_users)):\n",
    "    for ai in range(n_artworks):\n",
    "        user2artwork_mindist[ui][ai] = min(distmat[ai][j] for j in users[ui].artwork_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash(ui, pi, ni):\n",
    "    return  ((pi * n_artworks) + ni) * n_users + ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_instances = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "_collisions = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_instance(instance, pos_is_purchased=True):\n",
    "    ui, pi, ni = instance    \n",
    "    try:\n",
    "        assert 0 <= ui < n_users\n",
    "        assert 0 <= pi < n_artworks\n",
    "        assert 0 <= ni < n_artworks\n",
    "        assert pi != ni\n",
    "        user = users[ui]\n",
    "        if pos_is_purchased is True:\n",
    "            assert pi in user.artwork_idxs_set\n",
    "        else:\n",
    "            assert pi not in user.artwork_idxs_set\n",
    "        assert ni not in user.artwork_idxs_set\n",
    "        assert artist_ids[ni] not in user.artist_ids_set\n",
    "    except AssertionError:\n",
    "        print('ui = ', ui)\n",
    "        print('pi = ', pi)\n",
    "        print('ni = ', ni)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_instance(container, instance, **kwargs):    \n",
    "    global _collisions\n",
    "    h = hash(*instance)\n",
    "    if h in used_instances:\n",
    "        _collisions += 1\n",
    "        return False\n",
    "    sanity_check_instance(instance, **kwargs)\n",
    "    container.append(instance)\n",
    "    used_instances.add(h)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1) Each purchased item should trivially be ranked higher than any item of non-purchased artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_artwork_index__notsharingartist(profile_artist_ids):\n",
    "    while True:\n",
    "        i = random.randint(0, n_artworks-1)\n",
    "        if artist_ids[i] not in profile_artist_ids:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples__rank_purchased_above_nonpurchased(n_neg_per_pos=10, n_test_samples=5000):\n",
    "    \n",
    "    # --- train instances\n",
    "    print('sampling train instances ....')\n",
    "    for ui, user in enumerate(users):\n",
    "        u_artwork_idxs = user.artwork_idxs\n",
    "        u_artist_ids = user.artist_ids_set        \n",
    "        for pi in u_artwork_idxs:\n",
    "            for _ in range(n_neg_per_pos):\n",
    "                for __ in range(4):\n",
    "                    ni = sample_artwork_index__notsharingartist(u_artist_ids)\n",
    "                    if append_instance(train_instances, (ui, pi, ni)):\n",
    "                        break\n",
    "        \n",
    "    # --- test instances\n",
    "    print('sampling test instances ....')\n",
    "    while n_test_samples > 0:\n",
    "        ui = random.randint(0,n_users-1)\n",
    "        user = users[ui]\n",
    "        pi = random.choice(user.artwork_idxs)\n",
    "        ni = sample_artwork_index__notsharingartist(user.artist_ids_set)\n",
    "        if append_instance(test_instances, (ui, pi, ni)):\n",
    "            n_test_samples -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ....\n",
      "sampling test instances ....\n",
      "1395250 30000\n",
      "collisions =  14276\n"
     ]
    }
   ],
   "source": [
    "generate_samples__rank_purchased_above_nonpurchased(n_neg_per_pos=250, n_test_samples=30000)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('collisions = ', _collisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2) Given a user, any non-purchased item sharing the same artist with a purchased item should be ranked higher than any item of a non-purchased artist as long as ResNet50 doesn't disagree by much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_artwork_index__nonpurchased_sharingartist(artist_id, artwork_idxs_set):\n",
    "    candidate_idxs = artistId2artworkIndexes[artist_id]\n",
    "    for _ in range(10): # try at most 10 times\n",
    "        i = random.choice(candidate_idxs)\n",
    "        if i not in artwork_idxs_set:\n",
    "            return i\n",
    "    return None # failed to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_ui_pi_ni_triplet(ui, pi, ni, threshold=0.55):\n",
    "    dp = user2artwork_mindist[ui][pi]\n",
    "    dn = user2artwork_mindist[ui][ni]\n",
    "    assert dp + dn > 0\n",
    "    return dp / (dp + dn) > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_artwork_index__notsharingartist_tripletacceptable(ui, pi, threshold):\n",
    "    while True:\n",
    "        ni = sample_artwork_index__notsharingartist(users[ui].artist_ids_set)\n",
    "        if not reject_ui_pi_ni_triplet(ui, pi, ni, threshold=threshold):\n",
    "            return ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples__rank_purchased_artist_above_nonpurchased_artist(instances_container, n_samples_per_user=100):\n",
    "    for ui in range(n_users):\n",
    "        user = users[ui]\n",
    "        for _ in range(n_samples_per_user):\n",
    "            for __ in range(5):\n",
    "                aid = artist_ids[random.choice(user.artwork_idxs)]\n",
    "                assert aid != -1\n",
    "                pi = sample_artwork_index__nonpurchased_sharingartist(aid, user.artwork_idxs_set)\n",
    "                if pi is None:\n",
    "                    continue\n",
    "                ni = sample_artwork_index__notsharingartist_tripletacceptable(ui, pi, 0.55)\n",
    "                if append_instance(instances_container, (ui, pi, ni), pos_is_purchased=False):\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling train instances ...\n",
      "sampling test instances ...\n",
      "3114447 115954\n",
      "collisions =  18501\n"
     ]
    }
   ],
   "source": [
    "print('sampling train instances ...')\n",
    "generate_samples__rank_purchased_artist_above_nonpurchased_artist(train_instances, n_samples_per_user=600)\n",
    "print('sampling test instances ...')\n",
    "generate_samples__rank_purchased_artist_above_nonpurchased_artist(test_instances, n_samples_per_user=30)\n",
    "print(len(train_instances), len(test_instances))\n",
    "print('collisions = ', _collisions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tensorflow Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, n_users, n_items, user_latent_dim, item_latent_dim, item_visual_dim,\n",
    "                 pretrained_dim=2048,\n",
    "                 learning_rate=1e-4):\n",
    "        \n",
    "        assert (user_latent_dim == item_latent_dim + item_visual_dim)\n",
    "        \n",
    "        print('Network::__init__: learning_rate = ', learning_rate)\n",
    "        \n",
    "        self._item_visual_dim = item_visual_dim\n",
    "        \n",
    "        # --- placeholders\n",
    "        self._pretrained_image_embeddings = tf.placeholder(shape=[None, pretrained_dim], dtype=tf.float32,\n",
    "                                                     name='pretrained_image_embeddings')    \n",
    "        self._user_index = tf.placeholder(shape=[None], dtype=tf.int32,\n",
    "                                          name='user_index')\n",
    "        self._positive_item_index = tf.placeholder(shape=[None], dtype=tf.int32,\n",
    "                                                   name='positive_item_index')\n",
    "        self._negative_item_index = tf.placeholder(shape=[None], dtype=tf.int32,\n",
    "                                                   name='negative_item_index')\n",
    "            \n",
    "        # ------------------------------------\n",
    "        # ---- Global trainable variables\n",
    "        \n",
    "        # -- user latent factor matrix\n",
    "        # (n_users x user_latent_dim)\n",
    "        self._user_latent_factors = tf.Variable(\n",
    "            tf.random_uniform([n_users, user_latent_dim], -1.0, 1.0),\n",
    "            name='user_latent_factors'\n",
    "        )\n",
    "        \n",
    "        # -- item latent factor matrix\n",
    "        # (n_items x item_latent_dim)\n",
    "        self._item_latent_factors = tf.Variable(\n",
    "            tf.random_uniform([n_items, item_latent_dim], -1.0, 1.0),\n",
    "            name='item_latent_factors'\n",
    "        )\n",
    "        \n",
    "        # -- item latent biases\n",
    "        self._item_latent_biases = tf.Variable(\n",
    "            tf.random_uniform([n_items], -1.0, 1.0),\n",
    "            name='item_latent_biases'\n",
    "        )\n",
    "        \n",
    "        # -- global visual bias\n",
    "        self._visual_bias = tf.Variable(\n",
    "            tf.random_uniform([pretrained_dim], -1.0, 1.0),\n",
    "            name='visual_bias'\n",
    "        )\n",
    "        \n",
    "        # -------------------------------\n",
    "        # ---- minibatch tensors\n",
    "        \n",
    "        # -- user\n",
    "        self._user_latent_vector = tf.gather(self._user_latent_factors, self._user_index)\n",
    "        \n",
    "        # -- positive item\n",
    "        self._pos_vector,\\\n",
    "        self._pos_latent_bias,\\\n",
    "        self._pos_visual_bias = self.get_item_variables(self._positive_item_index)\n",
    "        self._pos_score = tf.reduce_sum(self._user_latent_vector * self._pos_vector, 1) +\\\n",
    "                    self._pos_latent_bias +\\\n",
    "                    self._pos_visual_bias\n",
    "        \n",
    "        # -- negative item\n",
    "        self._neg_vector,\\\n",
    "        self._neg_latent_bias,\\\n",
    "        self._neg_visual_bias = self.get_item_variables(self._negative_item_index)\n",
    "        self._neg_score = tf.reduce_sum(self._user_latent_vector * self._neg_vector, 1) +\\\n",
    "                    self._neg_latent_bias +\\\n",
    "                    self._neg_visual_bias\n",
    "        \n",
    "        # -------------------------------\n",
    "        # ---- train-test tensors\n",
    "        \n",
    "        # -- train loss\n",
    "        delta_score = self._pos_score - self._neg_score\n",
    "        ones = tf.fill(tf.shape(self._user_latent_vector)[:1], 1.0)\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=delta_score, labels=ones)\n",
    "        loss = tf.reduce_mean(loss, name='train_loss')\n",
    "        self._train_loss = loss\n",
    "        \n",
    "        # -- test accuracy\n",
    "        accuracy = tf.reduce_sum(tf.cast(delta_score > .0, tf.float32), name = 'test_accuracy')\n",
    "        self._test_accuracy = accuracy\n",
    "        \n",
    "        # -- optimizer\n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self._train_loss)\n",
    "        \n",
    "    def get_item_variables(self, item_index):\n",
    "        pre_vector = tf.gather(self._pretrained_image_embeddings, item_index)\n",
    "        # 1) item vector\n",
    "        #    1.1) visual vector\n",
    "        visual_vector = self.trainable_image_embedding(pre_vector, self._item_visual_dim)\n",
    "        #    1.2) latent vector\n",
    "        latent_vector = tf.gather(self._item_latent_factors, item_index)\n",
    "        #    1.3) concatenation\n",
    "        final_vector = tf.concat([visual_vector, latent_vector], 1)\n",
    "        # 2) latent bias\n",
    "        latent_bias = tf.gather(self._item_latent_biases, item_index)\n",
    "        # 3) visual bias\n",
    "        visual_bias = tf.reduce_sum(pre_vector * self._visual_bias, 1)\n",
    "        # return\n",
    "        return final_vector, latent_bias, visual_bias\n",
    "        \n",
    "    @staticmethod\n",
    "    def trainable_image_embedding(X, output_dim):\n",
    "        with tf.variable_scope(\"trainable_image_embedding\", reuse=tf.AUTO_REUSE):\n",
    "            fc1 = tf.layers.dense( # None -> output_dim\n",
    "                inputs=X,\n",
    "                units=output_dim,\n",
    "                name='fc1'\n",
    "            )\n",
    "            return fc1\n",
    "    \n",
    "    def optimize_and_get_train_loss(self, sess, pretrained_image_embeddings,\n",
    "                                    user_index, positive_item_index, negative_item_index):\n",
    "        return sess.run([\n",
    "            self._optimizer,\n",
    "            self._train_loss,\n",
    "        ], feed_dict={\n",
    "            self._pretrained_image_embeddings: pretrained_image_embeddings,\n",
    "            self._user_index: user_index,\n",
    "            self._positive_item_index: positive_item_index,\n",
    "            self._negative_item_index: negative_item_index,\n",
    "        })\n",
    "    \n",
    "    def get_train_loss(self, sess, pretrained_image_embeddings, user_index, positive_item_index, negative_item_index):\n",
    "        return sess.run(\n",
    "            self._train_loss, feed_dict={\n",
    "            self._pretrained_image_embeddings: pretrained_image_embeddings,\n",
    "            self._user_index: user_index,\n",
    "            self._positive_item_index: positive_item_index,\n",
    "            self._negative_item_index: negative_item_index,\n",
    "        })\n",
    "    \n",
    "    def get_test_accuracy(self, sess, pretrained_image_embeddings, user_index, positive_item_index, negative_item_index):\n",
    "        return sess.run(\n",
    "            self._test_accuracy, feed_dict={\n",
    "            self._pretrained_image_embeddings: pretrained_image_embeddings,\n",
    "            self._user_index: user_index,\n",
    "            self._positive_item_index: positive_item_index,\n",
    "            self._negative_item_index: negative_item_index,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # DEBUGGING \n",
    "# with tf.Graph().as_default():\n",
    "#     network = Network(n_users=10, n_items=10, user_latent_dim=40, item_latent_dim=20,\n",
    "#                       item_visual_dim=20, pretrained_dim=2048)\n",
    "#     with tf.Session() as sess:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#         tmp_debug = sess.run([\n",
    "#             network._user_latent_factors,\n",
    "#             network._item_latent_factors,\n",
    "#             network._pos_vector,\n",
    "#             network._pos_latent_bias,\n",
    "#             network._pos_visual_bias,\n",
    "#             network._pos_score,\n",
    "#             network._neg_vector,\n",
    "#             network._neg_latent_bias,\n",
    "#             network._neg_visual_bias,\n",
    "#             network._neg_score,\n",
    "#             network._user_latent_vector,\n",
    "#         ], feed_dict={\n",
    "#             network._pretrained_image_embeddings: resnet50_embeddings,\n",
    "#             network._user_index: [0, 2, 5],\n",
    "#             network._positive_item_index: [1, 3, 9],\n",
    "#             network._negative_item_index: [2, 7, 8],\n",
    "#         })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_minibatches(tuples, batch_size):\n",
    "    n_tuples = len(tuples)\n",
    "    n_batches = (n_tuples // batch_size) + int(n_tuples % batch_size > 0)\n",
    "    \n",
    "    indexes = list(range(n_tuples))\n",
    "    random.shuffle(indexes)\n",
    "    \n",
    "    print('n_tuples = ', n_tuples)\n",
    "    print('n_batches = ', n_batches)\n",
    "    \n",
    "    user_index_batches = [None] * n_batches\n",
    "    pos_index_batches = [None] * n_batches\n",
    "    neg_index_batches = [None] * n_batches\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        jmin = i * batch_size\n",
    "        jmax = min(jmin + batch_size, n_tuples)\n",
    "        actual_batch_size = jmax - jmin\n",
    "        \n",
    "        user_index_batch = np.empty((actual_batch_size,), dtype=int)\n",
    "        pos_index_batch = np.empty((actual_batch_size,), dtype=int)\n",
    "        neg_index_batch = np.empty((actual_batch_size,), dtype=int)\n",
    "        \n",
    "        for j in range(actual_batch_size):\n",
    "            t = tuples[indexes[jmin+j]]\n",
    "            user_index_batch[j] = t[0]\n",
    "            pos_index_batch[j] = t[1]\n",
    "            neg_index_batch[j] = t[2]\n",
    "\n",
    "        user_index_batches[i] = user_index_batch\n",
    "        pos_index_batches[i] = pos_index_batch\n",
    "        neg_index_batches[i] = neg_index_batch\n",
    "        \n",
    "    return dict(\n",
    "        user_index_batches = user_index_batches,\n",
    "        pos_index_batches  = pos_index_batches,\n",
    "        neg_index_batches  = neg_index_batches,\n",
    "        n_batches               = n_batches,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_minibatches(minibatches):\n",
    "    user_index_batches = minibatches['user_index_batches']\n",
    "    pos_index_batches = minibatches['pos_index_batches']\n",
    "    neg_index_batches = minibatches['neg_index_batches']\n",
    "    n_batches = minibatches['n_batches']\n",
    "    assert n_batches == len(user_index_batches)\n",
    "    assert n_batches == len(pos_index_batches)\n",
    "    assert n_batches == len(neg_index_batches)\n",
    "    assert n_batches > 0\n",
    "    \n",
    "    for user_index, pos_index, neg_index in zip(\n",
    "        user_index_batches,\n",
    "        pos_index_batches,\n",
    "        neg_index_batches\n",
    "    ):\n",
    "        n = user_index.shape[0]\n",
    "        assert n == pos_index.shape[0]\n",
    "        assert n == neg_index.shape[0]\n",
    "        \n",
    "        for i in range(n):\n",
    "            ui = user_index[i]\n",
    "            pi = pos_index[i]\n",
    "            ni = neg_index[i]\n",
    "            assert pi != ni\n",
    "            assert ni not in users[ui].artwork_idxs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '/mnt/workspace/pamessina_models/ugallery/VBPR/v2_hidinglast/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network(train_instances, test_instances, batch_size=64, max_epochs=60,\n",
    "                  learning_rate=1e-4, early_stopping_epochs=5, min_elapsed_epochs_to_save=3, session_config=None):\n",
    "    \n",
    "    train_minibatches = generate_minibatches(train_instances, batch_size)\n",
    "    test_minibatches = generate_minibatches(test_instances, batch_size)\n",
    "    sanity_check_minibatches(train_minibatches)\n",
    "    sanity_check_minibatches(test_minibatches)\n",
    "    n_train_batches = train_minibatches['n_batches']\n",
    "    n_test_batches = test_minibatches['n_batches']\n",
    "    n_test_instances = len(test_instances)\n",
    "    \n",
    "    with tf.Graph().as_default():\n",
    "        network = Network(\n",
    "            n_users=n_users,\n",
    "            n_items=n_artworks,\n",
    "            user_latent_dim=128,\n",
    "            item_latent_dim=64,\n",
    "            item_visual_dim=64,\n",
    "            pretrained_dim=2048,\n",
    "            learning_rate=learning_rate,\n",
    "        )\n",
    "        with tf.Session(config=session_config) as sess:\n",
    "            try:\n",
    "                saver = tf.train.Saver()\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(MODEL_PATH))\n",
    "                print('model successfully restored from checkpoint!')\n",
    "            except ValueError:\n",
    "                print('no checkpoint found: initializing variables with random values')\n",
    "                os.makedirs(MODEL_PATH, exist_ok=True)\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # ========= BEFORE TRAINING ============\n",
    "            \n",
    "            initial_test_acc = 0.\n",
    "            for user_index, pos_index, neg_index in zip(\n",
    "                test_minibatches['user_index_batches'],\n",
    "                test_minibatches['pos_index_batches'],\n",
    "                test_minibatches['neg_index_batches']\n",
    "            ):\n",
    "                minibatch_test_acc = network.get_test_accuracy(\n",
    "                    sess, resnet50_embeddings, user_index, pos_index, neg_index)\n",
    "                initial_test_acc += minibatch_test_acc\n",
    "            initial_test_acc = (initial_test_acc / n_test_instances) * 100.\n",
    "\n",
    "            print(\"Before training: test_accuracy = %f%%\" % initial_test_acc)\n",
    "            \n",
    "            best_test_acc = initial_test_acc\n",
    "            last_improvement_epoch = -1\n",
    "            last_improvement_epoch_train_loss = None\n",
    "            last_save_epoch = -1\n",
    "\n",
    "            # ========= TRAINING ============\n",
    "            \n",
    "            print ('Starting training ...')\n",
    "\n",
    "            for epoch in range(max_epochs):\n",
    "                \n",
    "                start_time = time.time()\n",
    "\n",
    "                # --- training\n",
    "                epoch_train_loss = 0.\n",
    "                for user_index, pos_index, neg_index in zip(\n",
    "                    train_minibatches['user_index_batches'],\n",
    "                    train_minibatches['pos_index_batches'],\n",
    "                    train_minibatches['neg_index_batches']\n",
    "                ):\n",
    "                    _, minibatch_train_loss = network.optimize_and_get_train_loss(\n",
    "                        sess, resnet50_embeddings, user_index, pos_index, neg_index)\n",
    "                    epoch_train_loss += minibatch_train_loss\n",
    "                epoch_train_loss /= n_train_batches\n",
    "\n",
    "                # --- testing\n",
    "                epoch_test_acc = 0.\n",
    "                for user_index, pos_index, neg_index in zip(\n",
    "                    test_minibatches['user_index_batches'],\n",
    "                    test_minibatches['pos_index_batches'],\n",
    "                    test_minibatches['neg_index_batches']\n",
    "                ):\n",
    "                    minibatch_test_acc = network.get_test_accuracy(\n",
    "                        sess, resnet50_embeddings, user_index, pos_index, neg_index)\n",
    "                    epoch_test_acc += minibatch_test_acc\n",
    "                epoch_test_acc = (epoch_test_acc / n_test_instances) * 100.\n",
    "                \n",
    "                # elapsed time\n",
    "                elapsed_seconds = time.time() - start_time\n",
    "                \n",
    "                # --- check for improvements and update best model if necessary\n",
    "                print(\"epoch %d: train_loss = %.15f, test_accuracy = %f%%, elapsed_seconds = %f\" % (\n",
    "                        epoch, epoch_train_loss, epoch_test_acc, elapsed_seconds))                \n",
    "                if ((epoch_test_acc > best_test_acc) or (\n",
    "                    epoch_test_acc == best_test_acc and (\n",
    "                        last_improvement_epoch_train_loss is not None and\\\n",
    "                        epoch_train_loss < last_improvement_epoch_train_loss\n",
    "                    )\n",
    "                )) and (epoch - last_save_epoch >= min_elapsed_epochs_to_save or epoch == max_epochs-1):\n",
    "                    saver = tf.train.Saver()\n",
    "                    save_path = saver.save(sess, MODEL_PATH)\n",
    "                    last_save_epoch = epoch\n",
    "                    last_improvement_epoch = epoch\n",
    "                    last_improvement_epoch_train_loss = epoch_train_loss\n",
    "                    best_test_acc = epoch_test_acc                    \n",
    "                    print(\"   ** improvement detected: model saved to path \", save_path)\n",
    "                else:                    \n",
    "                    if (epoch - last_improvement_epoch >= early_stopping_epochs):\n",
    "                        print(\"   *** %d epochs with no improvements -> early stopping :(\" % early_stopping_epochs)\n",
    "                        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_network(train_instances, test_instances,\n",
    "              batch_size=10000,\n",
    "              max_epochs=2000,\n",
    "              learning_rate=1e-3,\n",
    "              early_stopping_epochs=7,\n",
    "              min_elapsed_epochs_to_save=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
